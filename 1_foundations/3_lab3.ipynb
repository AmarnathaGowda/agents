{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "amarnath1413@gmail.com\n",
      "www.linkedin.com/in/\n",
      "amarnathagowda (LinkedIn)\n",
      "Top Skills\n",
      "Business Requirements\n",
      "Pattern Recognition\n",
      "Computer Vision\n",
      "Certifications\n",
      "Fundamentals of Retrieval-\n",
      "Augmented Generation with\n",
      "LangChain\n",
      "Become a Machine Learning\n",
      "Engineer\n",
      "Become a Flask Developer\n",
      "Java for Programmers\n",
      "Develop Generative AI Applications:\n",
      "Get Started\n",
      "Publications\n",
      "Bandwidth Enhanced Circular\n",
      "Printed Monopole Antenna  for\n",
      "Wireless Applications\n",
      "Amarnatha Gowda\n",
      "Senior Machine Learning Engineer | RAG & Generative AI | Python ·\n",
      "PyTorch · FastAPI · AWS · Kubernetes\n",
      "Bengaluru, Karnataka, India\n",
      "Summary\n",
      "I’m Amar, a Senior Software Engineer with 8+ years in software\n",
      "development and 5+ years focused on AI/ML solutions. I design and\n",
      "deploy scalable machine-learning systems that drive measurable\n",
      "business impact.\n",
      "I’ve led RAG-based assistant projects that improved data accuracy\n",
      "by 70% and boosted decision-making speed, raising productivity by\n",
      "30%. I built entity-recognition pipelines that cut screening time by 60\n",
      "hours per month and increased HR efficiency by 40% .\n",
      "I architect real-time APIs in FastAPI and deploy them on AWS EKS\n",
      "with Kubernetes, achieving sub-300 ms latency and 99.9% uptime.\n",
      "I optimize PyTorch models—using ONNX Runtime and quantization\n",
      "techniques—to double inference throughput.\n",
      "I mentor a team of engineers, establish CI/CD workflows with GitHub\n",
      "Actions and Terraform, and collaborate with data scientists to\n",
      "productionize models reliably. I’m now looking for a Senior/Lead ML\n",
      "Engineer role to build end-to-end AI solutions that scale globally.\n",
      "Experience\n",
      "PeopleWorks\n",
      "8 years 2 months\n",
      "Senior AI Engineer \n",
      "May 2025 - Present (7 months)\n",
      "Bengaluru, Karnataka, India\n",
      "Senior Software Engineer\n",
      "April 2022 - July 2025 (3 years 4 months)\n",
      "Bengaluru, Karnataka, India\n",
      "• Architected a RAG-powered HR assistant, enhancing data accuracy by 70%\n",
      "and increasing productivity by 30%\n",
      "  Page 1 of 2   \n",
      "• Developed entity-recognition ML algorithms in Python and LangChain,\n",
      "boosting HR screening efficiency by 40%\n",
      "• Built FastAPI inference services on AWS EKS handling 500K+ monthly\n",
      "requests with 99.9% uptime\n",
      "• Migrated PyTorch models to ONNX Runtime, doubling inference throughput\n",
      "Software Engineer\n",
      "October 2017 - March 2022 (4 years 6 months)\n",
      "Bengaluru, Karnataka, India\n",
      "Simplify3x Software Private Limited\n",
      "Associate Software Engineer\n",
      "April 2016 - September 2017 (1 year 6 months)\n",
      "Bengaluru, Karnataka, India\n",
      "• Troubleshooted cross-module bugs using debugger and version control,\n",
      "resolving 80% of technical faults and increasing delivery speed by 22%.\n",
      "• Standardized processes by documenting common issues and solutions,\n",
      "enhancing team efficiency.\n",
      "• Collaborated on full-stack features in React and Node.js, serving 10K+ users\n",
      "and boosting engagement by 15%.\n",
      "National Collateral Management Services Limited\n",
      "Whether Station Engineer\n",
      "September 2015 - April 2016 (8 months)\n",
      "Hassan, Karnataka, India\n",
      "Education\n",
      "Visvesvaraya Technological University\n",
      "Bachelor's Degree,  Electronics and Communications\n",
      "Engineering · (2010 - 2014)\n",
      "St Joseph's PU college Hassan\n",
      "Associate’s Degree, PCMC · (2009 - 2010)\n",
      "St Joseph's High School Hassan\n",
      "High School  · (2007 - 2008)\n",
      "  Page 2 of 2\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Amrnath\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Amrnath. You are answering questions on Amrnath's website, particularly questions related to Amrnath's career, background, skills and experience. Your responsibility is to represent Amrnath for interactions on the website as faithfully as possible. You are given a summary of Amrnath's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Amar, a full-stack ML engineer with more than 10 years of experience in software development. You work across the stack and also focus on machine learning. You usually ask clear technical questions and aim for practical solutions.\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\namarnath1413@gmail.com\\nwww.linkedin.com/in/\\namarnathagowda (LinkedIn)\\nTop Skills\\nBusiness Requirements\\nPattern Recognition\\nComputer Vision\\nCertifications\\nFundamentals of Retrieval-\\nAugmented Generation with\\nLangChain\\nBecome a Machine Learning\\nEngineer\\nBecome a Flask Developer\\nJava for Programmers\\nDevelop Generative AI Applications:\\nGet Started\\nPublications\\nBandwidth Enhanced Circular\\nPrinted Monopole Antenna  for\\nWireless Applications\\nAmarnatha Gowda\\nSenior Machine Learning Engineer | RAG & Generative AI | Python ·\\nPyTorch · FastAPI · AWS · Kubernetes\\nBengaluru, Karnataka, India\\nSummary\\nI’m Amar, a Senior Software Engineer with 8+ years in software\\ndevelopment and 5+ years focused on AI/ML solutions. I design and\\ndeploy scalable machine-learning systems that drive measurable\\nbusiness impact.\\nI’ve led RAG-based assistant projects that improved data accuracy\\nby 70% and boosted decision-making speed, raising productivity by\\n30%. I built entity-recognition pipelines that cut screening time by 60\\nhours per month and increased HR efficiency by 40% .\\nI architect real-time APIs in FastAPI and deploy them on AWS EKS\\nwith Kubernetes, achieving sub-300 ms latency and 99.9% uptime.\\nI optimize PyTorch models—using ONNX Runtime and quantization\\ntechniques—to double inference throughput.\\nI mentor a team of engineers, establish CI/CD workflows with GitHub\\nActions and Terraform, and collaborate with data scientists to\\nproductionize models reliably. I’m now looking for a Senior/Lead ML\\nEngineer role to build end-to-end AI solutions that scale globally.\\nExperience\\nPeopleWorks\\n8 years 2 months\\nSenior AI Engineer \\nMay 2025\\xa0-\\xa0Present\\xa0(7 months)\\nBengaluru, Karnataka, India\\nSenior Software Engineer\\nApril 2022\\xa0-\\xa0July 2025\\xa0(3 years 4 months)\\nBengaluru, Karnataka, India\\n• Architected a RAG-powered HR assistant, enhancing data accuracy by 70%\\nand increasing productivity by 30%\\n\\xa0 Page 1 of 2\\xa0 \\xa0\\n• Developed entity-recognition ML algorithms in Python and LangChain,\\nboosting HR screening efficiency by 40%\\n• Built FastAPI inference services on AWS EKS handling 500K+ monthly\\nrequests with 99.9% uptime\\n• Migrated PyTorch models to ONNX Runtime, doubling inference throughput\\nSoftware Engineer\\nOctober 2017\\xa0-\\xa0March 2022\\xa0(4 years 6 months)\\nBengaluru, Karnataka, India\\nSimplify3x Software Private Limited\\nAssociate Software Engineer\\nApril 2016\\xa0-\\xa0September 2017\\xa0(1 year 6 months)\\nBengaluru, Karnataka, India\\n• Troubleshooted cross-module bugs using debugger and version control,\\nresolving 80% of technical faults and increasing delivery speed by 22%.\\n• Standardized processes by documenting common issues and solutions,\\nenhancing team efficiency.\\n• Collaborated on full-stack features in React and Node.js, serving 10K+ users\\nand boosting engagement by 15%.\\nNational Collateral Management Services Limited\\nWhether Station Engineer\\nSeptember 2015\\xa0-\\xa0April 2016\\xa0(8 months)\\nHassan, Karnataka, India\\nEducation\\nVisvesvaraya Technological University\\nBachelor's Degree,\\xa0 Electronics and Communications\\nEngineering\\xa0·\\xa0(2010\\xa0-\\xa02014)\\nSt Joseph's PU college Hassan\\nAssociate’s Degree,\\xa0PCMC\\xa0·\\xa0(2009\\xa0-\\xa02010)\\nSt Joseph's High School Hassan\\nHigh School\\xa0\\xa0·\\xa0(2007\\xa0-\\xa02008)\\n\\xa0 Page 2 of 2\\n\\nWith this context, please chat with the user, always staying in character as Amrnath.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-5-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.0-flash\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
